{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:00:58.104835Z",
     "start_time": "2025-11-12T04:00:49.885673Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:00:58.138662Z",
     "start_time": "2025-11-12T04:00:58.113759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:00:58.170558Z",
     "start_time": "2025-11-12T04:00:58.163878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# 设置http的代理和不代理的地址\n",
    "os.environ['HTTP_PROXY'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['NO_PROXY'] = \"http://127.0.0.1:11434\" #ollama的本地服务地址"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:01:03.306379Z",
     "start_time": "2025-11-12T04:00:58.220130Z"
    }
   },
   "source": [
    "## Read the ppdfs from the folder\n",
    "loader=PyPDFDirectoryLoader(\"papers\")\n",
    "\n",
    "documents=loader.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "final_documents=text_splitter.split_documents(documents)\n",
    "final_documents[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-02T00:38:11+00:00', 'author': '', 'keywords': '', 'moddate': '2021-06-02T00:38:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'papers/Paper4.pdf', 'total_pages': 44, 'page': 0, 'page_label': '1'}, page_content='Diffusion Models Beat GANs on Image Synthesis\\nPrafulla Dhariwal∗\\nOpenAI\\nprafulla@openai.com\\nAlex Nichol∗\\nOpenAI\\nalex@openai.com\\nAbstract\\nWe show that diffusion models can achieve image sample quality superior to the\\ncurrent state-of-the-art generative models. We achieve this on unconditional im-\\nage synthesis by ﬁnding a better architecture through a series of ablations. For\\nconditional image synthesis, we further improve sample quality with classiﬁer guid-\\nance: a simple, compute-efﬁcient method for trading off diversity for ﬁdelity using\\ngradients from a classiﬁer. We achieve an FID of 2.97 on ImageNet 128 ×128,\\n4.59 on ImageNet 256 ×256, and 7.72 on ImageNet 512 ×512, and we match\\nBigGAN-deep even with as few as 25 forward passes per sample, all while main-\\ntaining better coverage of the distribution. Finally, we ﬁnd that classiﬁer guidance\\ncombines well with upsampling diffusion models, further improving FID to 3.94')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:01:03.384719Z",
     "start_time": "2025-11-12T04:01:03.376100Z"
    }
   },
   "source": [
    "len(final_documents)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:01:11.022010Z",
     "start_time": "2025-11-12T04:01:03.421745Z"
    }
   },
   "source": [
    "## Embedding Using Huggingface\n",
    "huggingface_embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",      #sentence-transformers/all-MiniLM-l6-v2\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:01:11.286548Z",
     "start_time": "2025-11-12T04:01:11.031428Z"
    }
   },
   "source": [
    "import  numpy as np\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.28379381e-02 -2.95848940e-02 -1.65011603e-02 -6.97876606e-03\n",
      "  4.93186377e-02  3.56690958e-02 -7.25341365e-02 -6.28811345e-02\n",
      " -3.42325168e-03  2.99070086e-02  1.10447528e-02 -3.16040367e-02\n",
      "  3.81746106e-02  5.44714481e-02  1.93874687e-02  7.04315081e-02\n",
      " -8.44008289e-03 -1.88588444e-02  4.77212183e-02 -1.40712420e-02\n",
      "  1.00122914e-02 -4.74376492e-02  4.91017736e-02 -3.02354526e-02\n",
      "  6.28539827e-03  8.31162371e-03  7.16581494e-02 -6.14517853e-02\n",
      " -1.80430245e-02 -2.34286398e-01  4.32564057e-02  1.20113594e-02\n",
      "  4.18742485e-02 -3.28857899e-02 -2.13135747e-04 -6.92436355e-04\n",
      " -2.48747766e-02 -2.50455774e-02 -2.86248066e-02  4.37693819e-02\n",
      " -2.68596802e-02  1.10380836e-02 -6.44884631e-02 -2.78332494e-02\n",
      "  4.22788821e-02 -1.22138187e-02 -5.35549149e-02 -1.87554117e-02\n",
      " -5.26116565e-02 -1.19410744e-02  5.90018113e-04 -5.63122295e-02\n",
      " -1.46162538e-02  4.74096872e-02  1.33543573e-02  2.75178496e-02\n",
      "  6.82152212e-02  2.61006635e-02  3.57025117e-02  3.64448689e-02\n",
      " -1.32619971e-02  3.70330364e-02 -8.99114311e-02  2.20806412e-02\n",
      "  2.82449406e-02  3.18610631e-02  3.44932936e-02 -3.70716602e-02\n",
      " -2.92634461e-02  3.43936384e-02  6.86393771e-03  2.41995398e-02\n",
      " -1.43467361e-04  3.59854661e-02  1.35927945e-02 -8.56681075e-03\n",
      "  6.07740507e-02 -2.19793431e-03 -3.19561623e-02 -2.91599836e-02\n",
      "  4.45594601e-02  3.51538919e-02 -4.14086709e-04  4.01299214e-03\n",
      " -1.55441317e-04  3.83405238e-02 -1.16081489e-02 -4.40892316e-02\n",
      " -1.02288797e-02 -1.45508936e-02  5.87535789e-03  8.82555824e-03\n",
      " -4.15317826e-02  5.35221919e-02 -1.61069650e-02  1.05768805e-02\n",
      "  1.03531741e-02  6.53456477e-03  6.82450505e-03  4.00249720e-01\n",
      " -4.11433727e-02 -8.68023094e-03  3.15749831e-02  2.06451565e-02\n",
      " -5.38362600e-02 -5.19087091e-02 -1.98244993e-02  1.10517964e-02\n",
      " -1.00980587e-01  5.15171289e-02 -4.55505066e-02 -2.07429901e-02\n",
      " -2.50678021e-03  1.40257049e-02 -2.34368928e-02 -6.37016371e-02\n",
      "  4.85297851e-02  1.40265757e-02  2.44293734e-02 -3.84367481e-02\n",
      " -3.97002324e-02  9.89694241e-03  3.06029599e-02 -2.83311214e-02\n",
      "  2.23604236e-02 -3.96599621e-02 -5.48439510e-02  8.50033909e-02\n",
      "  2.07743421e-02  1.79298390e-02 -1.33012226e-02  5.28848805e-02\n",
      " -9.05954167e-02  3.64777222e-02 -6.21927530e-03 -2.78536667e-04\n",
      "  3.89129831e-03  3.77719826e-03 -4.13424037e-02  2.75920914e-03\n",
      " -7.70079112e-03  4.28116173e-02  3.55349556e-02 -1.25194741e-02\n",
      " -7.45386705e-02  8.14754888e-02 -3.91285866e-02  2.39412524e-02\n",
      " -3.13930586e-02 -7.64013082e-02  4.32705209e-02  7.14464858e-02\n",
      " -4.79083769e-02 -2.40630154e-02  2.99440827e-02  2.53095534e-02\n",
      "  1.45825336e-03  1.12439813e-02 -8.05962905e-02  4.19100635e-02\n",
      " -2.71202810e-02 -8.21824893e-02 -6.33811429e-02  1.03706591e-01\n",
      " -6.99094124e-03 -3.93997207e-02 -6.13301480e-03 -2.23297458e-02\n",
      " -6.04173634e-03  5.28382622e-02 -2.94446647e-02 -4.10338938e-02\n",
      " -7.85273872e-03  5.52448332e-02 -9.68434382e-04  7.24695437e-03\n",
      " -6.49732277e-02 -5.05936854e-02 -2.75004469e-02  3.85492183e-02\n",
      " -4.99698818e-02 -4.73003052e-02 -1.34678343e-02  2.99753901e-02\n",
      "  1.31114619e-02 -5.17301522e-02  1.52030988e-02 -3.92754339e-02\n",
      " -1.44506739e-02  3.15442607e-02  1.82403233e-02  4.45134416e-02\n",
      " -4.75835279e-02  4.31825072e-02 -1.82065759e-02 -2.09246241e-02\n",
      " -2.69985665e-02  3.04850563e-02 -3.05419005e-02 -3.58162038e-02\n",
      " -3.06760333e-02  9.58473235e-03 -4.64179255e-02 -5.25640994e-02\n",
      " -2.15198863e-02  8.66083149e-03  1.37654627e-02 -1.29508628e-02\n",
      "  2.58446042e-03  6.98707253e-02 -5.52094616e-02 -2.41108947e-02\n",
      "  3.58128622e-02  4.58049290e-02  2.47346386e-02  6.70442730e-03\n",
      "  1.44614065e-02  1.36226397e-02  3.72206280e-03  2.03988515e-03\n",
      "  1.01559162e-02 -7.11597651e-02 -4.73516546e-02 -3.01653624e-01\n",
      " -1.80697683e-02  4.66380417e-02  1.96926575e-02  8.21286663e-02\n",
      " -1.08355217e-01  1.43174129e-02  1.73821219e-03  8.53930041e-02\n",
      "  7.79784322e-02 -1.03860581e-02  3.40569913e-02  1.31956628e-02\n",
      "  1.44601008e-03  1.20346574e-02 -3.75018292e-03  1.61318723e-02\n",
      "  3.65351215e-02 -1.33084897e-02 -4.35617007e-02 -6.79024728e-03\n",
      "  1.66649371e-02  5.45138270e-02 -9.06236619e-02  6.00031205e-02\n",
      " -4.12771618e-03  1.52844653e-01 -1.90670695e-02  1.27568971e-02\n",
      "  1.56065598e-02 -1.15795815e-02  4.68926802e-02 -2.59663388e-02\n",
      "  8.54149752e-04  5.14250249e-02  1.40741356e-02  6.95954263e-02\n",
      " -1.50389187e-02 -3.12727429e-02 -1.29889091e-02 -2.28058379e-02\n",
      "  4.41551022e-02  1.86060220e-02 -1.34465888e-01 -4.39759903e-02\n",
      "  3.18316892e-02 -3.13238017e-02  8.69150646e-03 -4.84958403e-02\n",
      "  1.23610999e-02  3.42699960e-02 -3.63278985e-02  6.82013109e-02\n",
      " -3.17278653e-02 -5.14795743e-02 -4.33584116e-02 -9.22080725e-02\n",
      "  4.25463878e-02 -5.15569076e-02 -9.28697828e-03  1.15135871e-02\n",
      " -5.47573715e-02 -7.12693343e-03 -4.15045954e-02  1.27996998e-02\n",
      "  3.16938497e-02  1.82895549e-02 -6.10908270e-02  1.16208661e-02\n",
      " -3.19775776e-03  5.17850667e-02  6.61677867e-02 -4.17807847e-02\n",
      "  6.58870861e-02  8.54525864e-02  3.16764265e-02 -3.25619578e-02\n",
      " -6.58412138e-03 -3.53158899e-02 -5.60618332e-03  2.99259201e-02\n",
      "  3.70001630e-03  7.49701411e-02  4.56278063e-02  3.50583568e-02\n",
      "  2.44524721e-02  4.40558828e-02 -5.17959939e-03 -1.74705265e-03\n",
      "  3.01295761e-02  5.91937359e-03  5.50488010e-02 -6.29061386e-02\n",
      " -2.95437798e-02  1.39172627e-02  1.36248600e-02 -2.38663152e-01\n",
      " -9.32573248e-03  5.76689318e-02  8.10435638e-02 -4.19735238e-02\n",
      "  4.56969552e-02  4.89828512e-02 -2.04052380e-03 -6.71941135e-03\n",
      "  3.04247788e-03 -8.35506432e-03  2.70032305e-02  7.70272389e-02\n",
      "  3.49327736e-02  3.05858254e-02  7.99321628e-04  3.88773344e-02\n",
      " -2.41822843e-02  4.89043482e-02 -1.56250373e-02 -1.77380554e-02\n",
      "  2.42665559e-02  1.61167040e-01 -9.91964787e-02 -4.83689234e-02\n",
      " -4.54960428e-02  1.49489325e-02 -4.37679999e-02 -8.04066006e-03\n",
      " -5.14003560e-02 -1.93409882e-02  2.97160670e-02  8.20508897e-02\n",
      " -1.42604122e-02  1.60297565e-02  8.94453973e-02 -1.50881363e-02\n",
      "  1.75994914e-02 -2.99318228e-02 -2.71263849e-02  2.06660703e-02\n",
      "  2.97531262e-02 -2.30966415e-03  5.91886751e-02 -1.69984084e-02\n",
      " -1.70676075e-02  1.57511979e-02  5.24739251e-02 -2.69795079e-02\n",
      " -1.69738568e-02  2.04616343e-03  3.01245670e-03  1.67369209e-02\n",
      " -7.53504457e-03  3.55796330e-02  5.35767851e-03 -2.14008782e-02\n",
      " -3.92198935e-02 -3.74687021e-03  3.56704853e-02  1.48158548e-02\n",
      " -1.21273324e-02  1.10807968e-02  1.54293794e-03 -7.59039149e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:01:34.834465Z",
     "start_time": "2025-11-12T04:01:11.296650Z"
    }
   },
   "source": [
    "## VectorStore Creation\n",
    "vectorstore=FAISS.from_documents(final_documents[:120],huggingface_embeddings)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:01:34.918821Z",
     "start_time": "2025-11-12T04:01:34.845185Z"
    }
   },
   "source": [
    "## Query using Similarity Search\n",
    "query=\"WHAT IS HEALTH INSURANCE COVERAGE?\"\n",
    "relevant_docments=vectorstore.similarity_search(query)\n",
    "\n",
    "print(relevant_docments[0].page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model FID sFID Prec Rec\n",
      "LSUN Bedrooms 256×256\n",
      "DCTransformer†[42] 6.40 6.66 0.44 0.56\n",
      "DDPM [25] 4.89 9.07 0.60 0.45\n",
      "IDDPM [43] 4.24 8.21 0.62 0.46\n",
      "StyleGAN [27] 2.35 6.62 0.59 0.48\n",
      "ADM (dropout) 1.90 5.59 0.66 0.51\n",
      "LSUN Horses 256×256\n",
      "StyleGAN2 [28] 3.84 6.46 0.63 0.48\n",
      "ADM 2.95 5.94 0.69 0.55\n",
      "ADM (dropout) 2.57 6.81 0.71 0.55\n",
      "LSUN Cats 256×256\n",
      "DDPM [25] 17.1 12.4 0.53 0.48\n",
      "StyleGAN2 [28] 7.25 6.33 0.58 0.43\n",
      "ADM (dropout) 5.57 6.69 0.63 0.52\n",
      "ImageNet 64×64\n",
      "BigGAN-deep* [5] 4.06 3.96 0.79 0.48\n",
      "IDDPM [43] 2.92 3.79 0.74 0.62\n",
      "ADM 2.61 3.77 0.73 0.63\n",
      "ADM (dropout) 2.07 4.29 0.74 0.63\n",
      "Model FID sFID Prec Rec\n",
      "ImageNet 128×128\n",
      "BigGAN-deep [5] 6.02 7.18 0.86 0.35\n",
      "LOGAN†[68] 3.36\n",
      "ADM 5.91 5.09 0.70 0.65\n",
      "ADM-G (25 steps) 5.98 7.04 0.78 0.51\n",
      "ADM-G 2.97 5.09 0.78 0.59\n",
      "ImageNet 256×256\n",
      "DCTransformer†[42] 36.51 8.24 0.36 0.67\n",
      "VQ-V AE-2†‡[51] 31.11 17.38 0.36 0.57\n",
      "IDDPM‡[43] 12.26 5.42 0.70 0.62\n",
      "SR3†‡[53] 11.30\n",
      "BigGAN-deep [5] 6.95 7.36 0.87 0.28\n",
      "ADM 10.94 6.02 0.69 0.63\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:01:34.962353Z",
     "start_time": "2025-11-12T04:01:34.955906Z"
    }
   },
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x715357844090> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:05:35.538777Z",
     "start_time": "2025-11-12T04:05:34.223758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import ModelCard,model_info\n",
    "\n",
    "card = ModelCard.load(\"moonshotai/Kimi-K2-Thinking\")\n",
    "print(card.data)\n",
    "print(model_info(\"moonshotai/Kimi-K2-Thinking\").pipeline_tag)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library_name: transformers\n",
      "license: other\n",
      "license_name: modified-mit\n",
      "text-generation\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:05:48.472957Z",
     "start_time": "2025-11-12T04:05:47.355863Z"
    }
   },
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "## 这种方法已经不可用，解决方案参考 test.py\n",
    "hf=HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    ")\n",
    "query=\"What is the health insurance coverage?\"\n",
    "# hf.invoke(query)\n",
    "hf.invoke(query)\n",
    "# hf.get_num_tokens(query)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model meta-llama/Llama-3.1-8B-Instruct is not supported for task text-generation and provider novita. Supported task: conversational.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      7\u001B[39m query=\u001B[33m\"\u001B[39m\u001B[33mWhat is the health insurance coverage?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# hf.invoke(query)\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[43mhf\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# hf.get_num_tokens(query)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:373\u001B[39m, in \u001B[36mBaseLLM.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    362\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    363\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    364\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    369\u001B[39m     **kwargs: Any,\n\u001B[32m    370\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    371\u001B[39m     config = ensure_config(config)\n\u001B[32m    372\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    374\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    375\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    376\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    377\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    378\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    380\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    381\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    382\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    383\u001B[39m         .generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m    384\u001B[39m         .text\n\u001B[32m    385\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:784\u001B[39m, in \u001B[36mBaseLLM.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    775\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    776\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    777\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    781\u001B[39m     **kwargs: Any,\n\u001B[32m    782\u001B[39m ) -> LLMResult:\n\u001B[32m    783\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1006\u001B[39m, in \u001B[36mBaseLLM.generate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    987\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m    988\u001B[39m     run_managers = [\n\u001B[32m    989\u001B[39m         callback_manager.on_llm_start(\n\u001B[32m    990\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1004\u001B[39m         )\n\u001B[32m   1005\u001B[39m     ]\n\u001B[32m-> \u001B[39m\u001B[32m1006\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) > \u001B[32m0\u001B[39m:\n\u001B[32m   1014\u001B[39m     run_managers = [\n\u001B[32m   1015\u001B[39m         callback_managers[idx].on_llm_start(\n\u001B[32m   1016\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1023\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m missing_prompt_idxs\n\u001B[32m   1024\u001B[39m     ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:810\u001B[39m, in \u001B[36mBaseLLM._generate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m    799\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate_helper\u001B[39m(\n\u001B[32m    800\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    801\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    806\u001B[39m     **kwargs: Any,\n\u001B[32m    807\u001B[39m ) -> LLMResult:\n\u001B[32m    808\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    809\u001B[39m         output = (\n\u001B[32m--> \u001B[39m\u001B[32m810\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    811\u001B[39m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    812\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    813\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[32m    814\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    815\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    816\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    817\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    818\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._generate(prompts, stop=stop)\n\u001B[32m    819\u001B[39m         )\n\u001B[32m    820\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    821\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1500\u001B[39m, in \u001B[36mLLM._generate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1497\u001B[39m new_arg_supported = inspect.signature(\u001B[38;5;28mself\u001B[39m._call).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1498\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[32m   1499\u001B[39m     text = (\n\u001B[32m-> \u001B[39m\u001B[32m1500\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1501\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m   1502\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(prompt, stop=stop, **kwargs)\n\u001B[32m   1503\u001B[39m     )\n\u001B[32m   1504\u001B[39m     generations.append([Generation(text=text)])\n\u001B[32m   1505\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations=generations)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:317\u001B[39m, in \u001B[36mHuggingFaceEndpoint._call\u001B[39m\u001B[34m(self, prompt, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m         completion += chunk.text\n\u001B[32m    315\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m completion\n\u001B[32m--> \u001B[39m\u001B[32m317\u001B[39m response_text = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtext_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    318\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    319\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    320\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minvocation_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    321\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    323\u001B[39m \u001B[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001B[39;00m\n\u001B[32m    324\u001B[39m \u001B[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001B[39;00m\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m stop_seq \u001B[38;5;129;01min\u001B[39;00m invocation_params[\u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:2357\u001B[39m, in \u001B[36mInferenceClient.text_generation\u001B[39m\u001B[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[39m\n\u001B[32m   2355\u001B[39m model_id = model \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model\n\u001B[32m   2356\u001B[39m provider_helper = get_provider_helper(\u001B[38;5;28mself\u001B[39m.provider, task=\u001B[33m\"\u001B[39m\u001B[33mtext-generation\u001B[39m\u001B[33m\"\u001B[39m, model=model_id)\n\u001B[32m-> \u001B[39m\u001B[32m2357\u001B[39m request_parameters = \u001B[43mprovider_helper\u001B[49m\u001B[43m.\u001B[49m\u001B[43mprepare_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mextra_payload\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2361\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2362\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2363\u001B[39m \u001B[43m    \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2364\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2366\u001B[39m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n\u001B[32m   2367\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_providers/_common.py:93\u001B[39m, in \u001B[36mTaskProviderHelper.prepare_request\u001B[39m\u001B[34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001B[39m\n\u001B[32m     90\u001B[39m api_key = \u001B[38;5;28mself\u001B[39m._prepare_api_key(api_key)\n\u001B[32m     92\u001B[39m \u001B[38;5;66;03m# mapped model from HF model ID\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m provider_mapping_info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prepare_mapping_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001B[39;00m\n\u001B[32m     96\u001B[39m headers = \u001B[38;5;28mself\u001B[39m._prepare_headers(headers, api_key)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/gitlab.chesterwang.com/ai_demo_4_interview/RAG/RAG-Level-01/P01-RAG-Projects/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_providers/_common.py:171\u001B[39m, in \u001B[36mTaskProviderHelper._prepare_mapping_info\u001B[39m\u001B[34m(self, model)\u001B[39m\n\u001B[32m    168\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m is not supported by provider \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.provider\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m provider_mapping.task != \u001B[38;5;28mself\u001B[39m.task:\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    172\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m is not supported for task \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.task\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m and provider \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.provider\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    173\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSupported task: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprovider_mapping.task\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    174\u001B[39m     )\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m provider_mapping.status == \u001B[33m\"\u001B[39m\u001B[33mstaging\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    177\u001B[39m     logger.warning(\n\u001B[32m    178\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m is in staging mode for provider \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.provider\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. Meant for test purposes only.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    179\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Model meta-llama/Llama-3.1-8B-Instruct is not supported for task text-generation and provider novita. Supported task: conversational."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:07:38.117226Z",
     "start_time": "2025-11-12T04:06:30.778270Z"
    }
   },
   "source": [
    "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "#这种方法是下载模型到本地进行运行\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"Qwen/Qwen3-0.6B\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300},\n",
    "    model_kwargs={\"trust_remote_code\": True},\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "llm.invoke(query)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the health insurance coverage? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the difference between health insurance and health care? What is the'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:07:51.655191Z",
     "start_time": "2025-11-12T04:07:51.652712Z"
    }
   },
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    " \"\"\""
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:07:57.591370Z",
     "start_time": "2025-11-12T04:07:57.588499Z"
    }
   },
   "source": [
    "# prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])\n",
    "prompt=PromptTemplate.from_template(prompt_template)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:08:00.313929Z",
     "start_time": "2025-11-12T04:08:00.236802Z"
    }
   },
   "source": [
    "retrievalQA=RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":prompt}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:08:02.941975Z",
     "start_time": "2025-11-12T04:08:02.939911Z"
    }
   },
   "source": [
    "query=\"\"\"DIFFERENCES IN THE\n",
    "UNINSURED RATE BY STATE\n",
    "IN 2022\"\"\""
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:09:35.624645Z",
     "start_time": "2025-11-12T04:08:08.917906Z"
    }
   },
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context\n",
      "\n",
      "ˆq(xt+1|xt):\n",
      "ˆq(xt+1|xt) =\n",
      "∫\n",
      "y\n",
      "ˆq(xt+1,y|xt) dy (32)\n",
      "=\n",
      "∫\n",
      "y\n",
      "ˆq(xt+1|xt,y)ˆq(y|xt) dy (33)\n",
      "=\n",
      "∫\n",
      "y\n",
      "q(xt+1|xt)ˆq(y|xt) dy (34)\n",
      "= q(xt+1|xt)\n",
      "∫\n",
      "y\n",
      "ˆq(y|xt) dy (35)\n",
      "= q(xt+1|xt) (36)\n",
      "= ˆq(xt+1|xt,y) (37)\n",
      "Following similar logic, we ﬁnd the joint distribution ˆq(x1:T|x0):\n",
      "ˆq(x1:T|x0) =\n",
      "∫\n",
      "y\n",
      "ˆq(x1:T,y|x0) dy (38)\n",
      "=\n",
      "∫\n",
      "y\n",
      "ˆq(y|x0)ˆq(x1:T|x0,y) dy (39)\n",
      "=\n",
      "∫\n",
      "y\n",
      "ˆq(y|x0)\n",
      "T∏\n",
      "t=1\n",
      "ˆq(xt|xt−1,y) dy (40)\n",
      "=\n",
      "∫\n",
      "y\n",
      "ˆq(y|x0)\n",
      "T∏\n",
      "t=1\n",
      "q(xt|xt−1) dy (41)\n",
      "=\n",
      "T∏\n",
      "t=1\n",
      "q(xt|xt−1)\n",
      "∫\n",
      "y\n",
      "ˆq(y|x0) dy (42)\n",
      "=\n",
      "T∏\n",
      "t=1\n",
      "q(xt|xt−1) (43)\n",
      "= q(x1:T|x0) (44)\n",
      "25\n",
      "\n",
      "producing higher ﬁdelity (but less diverse) samples.\n",
      "In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling\n",
      "p(x). It is also possible to train conditional diffusion models, p(x|y), and use classiﬁer guidance in\n",
      "the exact same way. Table 4 shows that the sample quality of both unconditional and conditional\n",
      "models can be greatly improved by classiﬁer guidance. We see that, with a high enough scale, the\n",
      "guided unconditional model can get quite close to the FID of an unguided conditional model, although\n",
      "training directly with the class labels still helps. Guiding a conditional model further improves FID.\n",
      "Table 4 also shows that classiﬁer guidance improves precision at the cost of recall, thus introducing\n",
      "a trade-off in sample ﬁdelity versus diversity. We explicitly evaluate how this trade-off varies with\n",
      "8\n",
      "\n",
      "Using Equation 44, we can now derive ˆq(xt):\n",
      "ˆq(xt) =\n",
      "∫\n",
      "x0:t−1\n",
      "ˆq(x0,...,x t) dx0:t−1 (45)\n",
      "=\n",
      "∫\n",
      "x0:t−1\n",
      "ˆq(x0)ˆq(x1,...,x t|x0) dx0:t−1 (46)\n",
      "=\n",
      "∫\n",
      "x0:t−1\n",
      "q(x0)q(x1,...,x t|x0) dx0:t−1 (47)\n",
      "=\n",
      "∫\n",
      "x0:t−1\n",
      "q(x0,...,x t) dx0:t−1 (48)\n",
      "= q(xt) (49)\n",
      "(50)\n",
      "Using the identities ˆq(xt) = q(xt) and ˆq(xt+1|xt) = q(xt+1|xt), it is trivial to show via Bayes rule\n",
      "that the unconditional reverse process ˆq(xt|xt+1) = q(xt|xt+1).\n",
      "One observation about ˆqis that it gives rise to a noisy classiﬁcation function, ˆq(y|xt). We can show\n",
      "that this classiﬁcation distribution does not depend on xt+1 (a noisier version of xt), a fact which we\n",
      "will later use:\n",
      "ˆq(y|xt,xt+1) = ˆq(xt+1|xt,y) ˆq(y|xt)\n",
      "ˆq(xt+1|xt) (51)\n",
      "= ˆq(xt+1|xt) ˆq(y|xt)\n",
      "ˆq(xt+1|xt) (52)\n",
      "= ˆq(y|xt) (53)\n",
      "(54)\n",
      "We can now derive the conditional reverse process:\n",
      "ˆq(xt|xt+1,y) = ˆq(xt,xt+1,y)\n",
      "ˆq(xt+1,y) (55)\n",
      "= ˆq(xt,xt+1,y)\n",
      "ˆq(y|xt+1)ˆq(xt+1) (56)\n",
      "= ˆq(xt|xt+1)ˆq(y|xt,xt+1)ˆq(xt+1)\n",
      "ˆq(y|xt+1)ˆq(xt+1) (57)\n",
      "= ˆq(xt|xt+1)ˆq(y|xt,xt+1)\n",
      "ˆq(y|xt+1) (58)\n",
      "Question:DIFFERENCES IN THE\n",
      "UNINSURED RATE BY STATE\n",
      "IN 2022\n",
      "\n",
      "Helpful Answers:\n",
      " 1. The uninsured rate is the percentage of people who are not covered by insurance in a given state. 2. The uninsured rate is the percentage of people who are not covered by insurance in a given state. 3. The uninsured rate is the percentage of people who are not covered by insurance in a given state. 4. The uninsured rate is the percentage of people who are not covered by insurance in a given state.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3, 4.\n",
      "Answer:\n",
      "The answer is: 1, 2, 3,\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
